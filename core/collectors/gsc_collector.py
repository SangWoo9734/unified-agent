"""
Google Search Console Data Collector

ì´ ëª¨ë“ˆì€ Google Search Console APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional

from google.oauth2 import service_account
from googleapiclient.discovery import build
import pandas as pd


class GSCCollector:
    """Google Search Console ë°ì´í„° ìˆ˜ì§‘ê¸°"""

    def __init__(self, credentials_path: str, property_url: str):
        """
        Args:
            credentials_path: ì„œë¹„ìŠ¤ ê³„ì • JSON í‚¤ íŒŒì¼ ê²½ë¡œ
            property_url: GSC ì†ì„± URL (ì˜ˆ: https://convertkits.org)
        """
        self.property_url = property_url
        self.credentials = service_account.Credentials.from_service_account_file(
            credentials_path,
            scopes=['https://www.googleapis.com/auth/webmasters.readonly']
        )
        self.service = build('searchconsole', 'v1', credentials=self.credentials)

    def fetch_search_analytics(
        self,
        days: int = 7,
        dimensions: Optional[List[str]] = None
    ) -> pd.DataFrame:
        """
        ê²€ìƒ‰ ë¶„ì„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

        Args:
            days: ê°€ì ¸ì˜¬ ì¼ìˆ˜ (ê¸°ë³¸: 7ì¼)
            dimensions: ì°¨ì› ëª©ë¡ (ê¸°ë³¸: ['query', 'page'])

        Returns:
            pandas DataFrame with search analytics data
        """
        if dimensions is None:
            dimensions = ['query', 'page']

        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚° (GSCëŠ” 3ì¼ ì „ ë°ì´í„°ê¹Œì§€ë§Œ ì•ˆì •ì )
        end_date = datetime.now() - timedelta(days=3)
        start_date = end_date - timedelta(days=days)

        request_body = {
            'startDate': start_date.strftime('%Y-%m-%d'),
            'endDate': end_date.strftime('%Y-%m-%d'),
            'dimensions': dimensions,
            'rowLimit': 1000,  # ìµœëŒ€ 1000ê°œ í–‰
            'startRow': 0
        }

        try:
            response = self.service.searchanalytics().query(
                siteUrl=self.property_url,
                body=request_body
            ).execute()

            if 'rows' not in response:
                print(f"âš ï¸  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ ({start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')})")
                return pd.DataFrame()

            # ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            rows = []
            for row in response['rows']:
                data = {
                    'query': row['keys'][0] if len(dimensions) > 0 else None,
                    'page': row['keys'][1] if len(dimensions) > 1 else None,
                    'clicks': row['clicks'],
                    'impressions': row['impressions'],
                    'ctr': row['ctr'],
                    'position': row['position']
                }
                rows.append(data)

            df = pd.DataFrame(rows)

            print(f"âœ… {len(df)}ê°œì˜ ê²€ìƒ‰ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            print(f"   ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")

            return df

        except Exception as e:
            print(f"âŒ GSC ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            raise

    def get_top_queries(self, df: pd.DataFrame, limit: int = 20) -> pd.DataFrame:
        """í´ë¦­ìˆ˜ ìƒìœ„ ê²€ìƒ‰ì–´ ì¶”ì¶œ"""
        return df.nlargest(limit, 'clicks')

    def get_opportunity_keywords(self, df: pd.DataFrame, limit: int = 10) -> pd.DataFrame:
        """
        ê¸°íšŒ í‚¤ì›Œë“œ ì°¾ê¸° (ë…¸ì¶œì€ ë§ì§€ë§Œ í´ë¦­ë¥ ì´ ë‚®ì€ í‚¤ì›Œë“œ)

        Args:
            df: ê²€ìƒ‰ ë°ì´í„° DataFrame
            limit: ë°˜í™˜í•  í‚¤ì›Œë“œ ìˆ˜

        Returns:
            ê¸°íšŒ í‚¤ì›Œë“œ DataFrame
        """
        # ìµœì†Œ ë…¸ì¶œ ìˆ˜ ì¡°ê±´ (ë…¸ì¶œ 100 ì´ìƒ)
        df_filtered = df[df['impressions'] >= 100].copy()

        # CTRì´ ë‚®ì€ ìˆœì„œë¡œ ì •ë ¬
        opportunities = df_filtered.nsmallest(limit, 'ctr')

        return opportunities

    def get_page_performance(self, df: pd.DataFrame) -> pd.DataFrame:
        """í˜ì´ì§€ë³„ ì„±ê³¼ ì§‘ê³„"""
        if df.empty:
            return pd.DataFrame()

        page_stats = df.groupby('page').agg({
            'clicks': 'sum',
            'impressions': 'sum',
            'position': 'mean'
        }).reset_index()

        page_stats['ctr'] = page_stats['clicks'] / page_stats['impressions']
        page_stats = page_stats.sort_values('clicks', ascending=False)

        return page_stats


if __name__ == '__main__':
    """í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰ ì½”ë“œ"""
    import sys
    from dotenv import load_dotenv

    load_dotenv()

    credentials_path = os.path.join(os.path.dirname(__file__), '..', 'config', 'gsc_credentials.json')
    property_url = os.getenv('GSC_PROPERTY_URL', 'https://convertkits.org')

    if not os.path.exists(credentials_path):
        print(f"âŒ ì¸ì¦ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {credentials_path}")
        sys.exit(1)

    collector = GSCCollector(credentials_path, property_url)
    df = collector.fetch_search_analytics(days=7)

    if not df.empty:
        print("\nğŸ“Š ìƒìœ„ 10ê°œ ê²€ìƒ‰ì–´:")
        print(collector.get_top_queries(df, limit=10))

        print("\nğŸ’ ê¸°íšŒ í‚¤ì›Œë“œ:")
        print(collector.get_opportunity_keywords(df, limit=5))
